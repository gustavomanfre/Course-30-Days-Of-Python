import os
import json
import csv
import re
from collections import Counter
#sys.path.append("data")
#from stop_words import stop_words as sw # Used in Level 2-7

# üíª Ejercicios: D√≠a 19 - Manejo de Archivos 

# ==========================================
# Ejercicios: Nivel 1
# ==========================================

# 1. Escriba una funci√≥n que cuente el n√∫mero de l√≠neas y el n√∫mero de palabras en un texto.
# Archivos: obama_speech.txt, michelle_obama_speech.txt, donald_speech.txt, melina_trump_speech.txt 

# Define la funci√≥n que recibe una ruta (texto) y promete devolver una tupla de dos enteros

# 1. 'def' define la funci√≥n. 'file_path: str' es un "type hint" que indica que espera un texto. '-> tuple[int, int]' avisa que la funci√≥n devolver√° una tupla con dos n√∫meros enteros.
def count_lines_words(file_path: str) -> tuple[int, int]:
    
    # 2. 'with' es un manejador de contexto que asegura que el archivo se cierre solo al terminar.
    # 'open' abre el archivo en modo lectura ("r") y 'as f' lo renombra como la variable 'f'.
    with open(file_path, "r") as f:
        
        # 3. 'f.readlines()' lee todo el archivo y guarda cada l√≠nea como un elemento de una lista.
        # 'lines: list[str]' es un "type hint" que aclara que 'lines' ser√° una lista de cadenas de texto.
        lines: list[str] = f.readlines()
        
        # 4. 'return' entrega los resultados. 'len(lines)' cuenta cu√°ntos elementos (l√≠neas) hay en la lista.
        # La coma ',' crea autom√°ticamente la tupla de retorno.
        # 'sum(...)' sumar√° los resultados de la expresi√≥n interna.
        # 'for line in lines' recorre cada l√≠nea.
        # 'line.split()' divide la l√≠nea en palabras (usando espacios) y 'len()' cuenta esas palabras.
        #(expresi√≥n for elemento in iterable)
        return len(lines), sum(len(line.split()) for line in lines)
    
    
# print(f"Obama: {count_lines_and_words('./data/obama_speech.txt')}")
# print(f"Michelle: {count_lines_and_words('./data/michelle_obama_speech.txt')}")
# print(f"Donald: {count_lines_and_words('./data/donald_speech.txt')}")
# print(f"Melina: {count_lines_and_words('./data/melina_trump_speech.txt')}")


# 2. Lea el archivo countries_data.json y encuentre los diez idiomas m√°s hablados.

# 1. 'def' define la funci√≥n. 'file_path: str' es la ruta del JSON y 'n: int' es cu√°ntos idiomas queremos ver.
def most_spoken_languages(file_path: str, n: int):
    
    # 2. Abrimos el archivo en modo lectura ('r'). 'encoding="utf-8"' es vital para leer nombres de idiomas con tildes o caracteres especiales.
    with open(file_path, "r", encoding="utf-8") as f:
        # 3. 'json.load(f)' transforma el texto del archivo JSON en una lista de diccionarios de Python y la guarda en 'countries'.
        countries = json.load(f)
    
    # 4. Inicializamos un diccionario vac√≠o donde la 'llave' ser√° el idioma y el 'valor' ser√° cu√°ntas veces aparece.
    language_counts: dict[str, int] = {}
    
    # 5. Esta es una "List Comprehension" anidada: recorre cada 'country' en 'countries', 
    # y por cada pa√≠s, recorre su lista de 'languages', extrayendo cada 'lang' individual a una lista plana.
    all_languages = [lang for country in countries for lang in country["languages"]]

    # 6. Iteramos sobre nuestra gran lista de idiomas (donde hay muchos repetidos).
    for lang in all_languages:
        # 7. 'language_counts.get(lang, 0)' busca el idioma; si no existe, devuelve 0. 
        # Luego le suma 1 y actualiza el diccionario. As√≠ evitamos errores de "llave no encontrada".
        # language_counts[espaniol] = language_counts.get(espaniol, 0) + 1, 
        # Esto dice que si el idioma ya existe, le suma 1 al valor almacenado; si no, lo inicializa en 0 y luego le suma 1.
        language_counts[lang] = language_counts.get(lang, 0) + 1

    # 8. Aqu√≠ pasan tres cosas:
    # A) '[(count, lang) for lang, count in language_counts.items()]' crea una lista de tuplas poniendo el n√∫mero PRIMERO para poder ordenar por cantidad.
    # B) 'sorted(..., reverse=True)' ordena la lista de mayor a menor frecuencia.
    # C) '[:n]' hace un "slicing" para devolver solo los primeros 'n' elementos (los m√°s hablados).
    # language_counts hasta esta l√≠nea es un diccionario como {'English': 91, 'French': 45, ...}
    # language_counts.items() convierten el diccionario en una lista de tuplas: [('English', 91), ('French', 45), ...]
    #Cada tupla es (lang, count), la recibe lang y count en el for y las invierte en (count, lang) para que sorted pueda ordenar por count
    #Como es una expresi√≥n de lista, crea una nueva lista con las tuplas invertidas (count, lang)
    # sorted(...) recibe una lista de tuplas y ordena esa lista de tuplas por el primer elemento de cada tupla (count) en orden descendente (reverse=True)
    return sorted([(count, lang) for lang, count in language_counts.items()], reverse=True)[:n]



# print(most_spoken_languages(filename='./data/countries_data.json', limit=10))
# print(most_spoken_languages(filename='./data/countries_data.json', limit=3))


# 3. Lea el archivo countries_data.json y cree una lista de los diez pa√≠ses m√°s poblados.

# 1. 'def' define la funci√≥n. Los "type hints" indican que devuelve una lista de diccionarios.
# 'int|str' significa que los valores del diccionario pueden ser n√∫meros o texto.
def most_populated_countries(file_path: str, n: int) -> list[dict[str, int|str]]:
    
    # 2. Abrimos el archivo en modo lectura ('r') con UTF-8 para asegurar que los nombres de pa√≠ses se lean bien.
    with open(file_path, "r", encoding="utf-8") as f:
        # 3. 'json.load(f)' convierte el contenido del JSON en una lista de diccionarios de Python.
        countries = json.load(f)

    # 4. Usamos "List Comprehension" para crear una NUEVA lista m√°s simple.
    # Por cada pa√≠s en la lista original, creamos un diccionario peque√±o solo con "country" y "population".
    # countries tiene diccionarios como {"name": "Afghanistan", "population": 40218234, ...}
    # Empezamos con countries pasando cada diccionario a country
    # Por cada country, creamos un nuevo diccionario {"country": country["name"], "population": country["population"]} y lo a√±adimos a population_list
    population_list: list[dict[str, int|str]] = [
        {"country": country["name"], "population": country["population"]} 
        for country in countries
    ]
    # -------------------------------------------------------------
    # AQU√ç OCURRE LA MAGIA DEL ORDENAMIENTO
    # -------------------------------------------------------------

    # sorted() es una funci√≥n incorporada de Python que:
    # 1) Recorre internamente la lista (NO necesitamos escribir un for)
    # 2) Ordena los elementos
    # 3) Devuelve una NUEVA lista ordenada (no modifica la original)

    # population_list:
    # Es la lista que queremos ordenar.
    # Contiene diccionarios como:
    # {"country": "China", "population": 1440000000}

    # key= :
    # key espera UNA FUNCI√ìN.
    # Esa funci√≥n se va a ejecutar UNA VEZ por cada elemento de la lista.
    #
    # Python hace internamente algo equivalente a:
    #
    # for x in population_list:
    #     valor = key(x)
    #     # usa ese valor para comparar y ordenar
    #
    # Nosotros NO escribimos ese for, sorted() lo hace por nosotros.

    # lambda x: x["population"]
    #
    # Esto es una funci√≥n an√≥nima (lambda).
    # - x representa CADA diccionario de la lista
    # - x["population"] obtiene el valor de poblaci√≥n de ese diccionario
    #
    # Ejemplo:
    # x = {"country": "China", "population": 1440000000}
    # x["population"] ‚Üí 1440000000
    #
    # Ese n√∫mero es el que Python usa para ordenar.

    # reverse=True:
    # - False (por defecto): orden ascendente (menor a mayor)
    # - True: orden descendente (mayor a menor)
    #
    # Como queremos los pa√≠ses M√ÅS poblados primero,
    # usamos reverse=True.

    # [:n]:
    # Una vez ordenada la lista,
    # cortamos la lista y devolvemos solo los primeros 'n' elementos.

    return sorted(
        population_list,                 # Lista de diccionarios a ordenar
        key=lambda x: x["population"],    # Funci√≥n que indica POR QU√â valor ordenar
        reverse=True                      # Orden descendente (mayor a menor)
    )[:n]                                # Devuelve solo los primeros n pa√≠ses.
                                        # sorted devuelve una lista, y el slicing [:n] corta esa lista.

# print(most_populated_countries(filename='./data/countries_data.json', limit=10))
# print(most_populated_countries(filename='./data/countries_data.json', limit=3))


# ==========================================
# Ejercicios: Nivel 2
# ==========================================

# 4. Extraiga todas las direcciones de correo electr√≥nico de email_exchange_big.txt. 
#
#  TODAVIA NO HE VISTO REGEX ASI QUE LO DEJO PARA MAS ADELANTE

import re # 1. ¬°Importante! Debes importar el m√≥dulo 're' para usar expresiones regulares.

# 2. Definimos la funci√≥n. 'file_path: str' es la ruta y '-> list[str]' indica que devolver√° una lista de textos (emails).
def get_email(file_path: str) -> list[str]:
    
    # 3. Abrimos el archivo en modo lectura ('r'). Usamos 'with' para que se cierre solo al terminar.
    with open(file_path, "r") as f:
        # 4. 'f.read()' lee TODO el contenido del archivo y lo guarda como una √∫nica cadena de texto gigante en 'raw_txt'.
        raw_txt: str = f.read()

    # 5. Aqu√≠ ocurre la magia de Regex:
    # 're.findall()' busca todas las coincidencias del patr√≥n en el texto y las devuelve en una lista.
    # El prefijo 'r' antes de las comillas indica que es una "raw string" (cadena en crudo), 
    # necesaria para que Python no confunda las barras invertidas de Regex con comandos especiales.
    
    # Desglose del patr√≥n r"[\w\.-]+@[\w\.-]+\.\w+":
    # [\w\.-]+  => Busca uno o m√°s caracteres que sean letras, n√∫meros, guiones bajos, puntos o guiones.
    # @         => Busca el s√≠mbolo '@' literal.
    # [\w\.-]+  => Busca el dominio (ej: 'gmail', 'outlook.co').
    # \.        => El punto es un car√°cter especial en Regex, as√≠ que '\.' le dice: "busca un punto real".
    # \w+       => Busca la terminaci√≥n (ej: 'com', 'org', 'es').
    all_emails = re.findall(r"[\w\.-]+@[\w\.-]+\.\w+", raw_txt)
    
    # 6. Devolvemos la lista con todos los correos electr√≥nicos encontrados en el archivo.
    return all_emails

# 5. Encuentra las palabras m√°s comunes en el idioma ingl√©s.
# Debe devolver una serie de tuplas en orden descendente.

# 1. Definimos la funci√≥n. Recibe la ruta y el n√∫mero 'n' de palabras deseadas.
# Retorna una lista de tuplas, donde cada tupla es (frecuencia, palabra).
def find_most_common_words(file_path: str, n: int) -> list[tuple]:
    
    # 2. Abrimos el archivo. 'f.read()' obtiene todo el texto.
    # '.lower()' convierte todo a min√∫sculas para que "Python" y "python" se cuenten como la misma palabra.
    with open(file_path, "r") as f:
        raw_text: str = f.read().lower()
    
    # 3. 're.findall' busca patrones. 
    # '\b' es un "word boundary" (l√≠mite de palabra): asegura que no capturemos letras dentro de otras palabras.
    # '[a-z]+' busca secuencias de una o m√°s letras (ignora n√∫meros y s√≠mbolos de puntuaci√≥n).
    all_words = re.findall(r"\b[a-z]+\b", raw_text)
    
    # 4. Creamos el diccionario de frecuencias.
    word_count: dict[str, int] = {}
    for word in all_words:
        # 5. Si la palabra ya existe, suma 1. Si no existe (.get devuelve 0), inicia en 1.
        word_count[word] = word_count.get(word, 0) + 1
    
    # 6. Transformaci√≥n y Ordenamiento:
    # A) Convertimos el diccionario en una lista de tuplas (count, word) mediante una List Comprehension.
    # B) 'sorted(..., reverse=True)' ordena de mayor a menor frecuencia.
    # C) '[:n]' corta la lista para devolver solo el "Top N".
    return sorted([(count, word) for word, count in word_count.items()], reverse=True)[:n]

# print(find_most_common_words('sample.txt', 10))

# The previous was my original approach. However, when refactoring my code, I came across this different approach using set(). This is significantly slower since list.count() runs in O(n) and is called for every unique word (O(n¬≤) overall). Nevertheless, I will leave it because I thought it was interesting
def find_most_common_wordsV2(file_path: str, n: int) -> list[tuple]:
    with open(file_path, "r") as f:
        raw_text:str = f.read().lower()
    
    all_words = re.findall(r"\b[a-z]+\b", raw_text)
    
    word_count = {word: all_words.count(word) for word in set(all_words)}
    
    return sorted([(count, word) for word, count in word_count.items()], reverse=True)[:n]


# 6. Utilice la funci√≥n anterior para encontrar las 10 palabras m√°s frecuentes en:
# 1. 'print()' es la funci√≥n de salida que enviar√° todo el texto procesado a la consola.
print(
    # 2. 'f' indica que es un f-string (cadena formateada), lo que permite insertar c√≥digo entre {}.
    # Usamos comillas dobles " " para envolver todo el bloque del f-string.
    # '\n' es un car√°cter de escape que inserta un "salto de l√≠nea" para que el texto sea legible.
    f"Obama's speech 10 most frequent words:\n"
    
    # 3. Entre llaves { } llamamos a la funci√≥n definida anteriormente.
    # IMPORTANTE: Usamos comillas simples './data/...' para la ruta del archivo.
    # Si us√°ramos dobles, Python pensar√≠a que el f-string termin√≥ ah√≠ y dar√≠a error.
    # Pasamos el archivo y el n√∫mero 10 para obtener las 10 palabras m√°s comunes.
    # \n\n" a√±ade dos saltos de l√≠nea para separar visualmente cada bloque de resultados.
    # Un ejemplo de separar visualmente los resultados, seria: palabras mas comunes de Obama, luego dos espacio, luego las de Michelle, etc.
    f"{find_most_common_words('./data/obama_speech.txt', 10)}\n\n"
    
    # 4. Repetimos el proceso para el discurso de Michelle.
    # El f-string eval√∫a la funci√≥n, obtiene la lista de tuplas y la convierte en texto autom√°ticamente.
    f"Michelle's speech 10 most frequent words:\n"
    f"{find_most_common_words('./data/michelle_obama_speech.txt', 10)}\n\n"
    
    # 5. Ejecuci√≥n para el discurso de Trump. 
    # Cada llamada a la funci√≥n abre el archivo, lo limpia, cuenta palabras y devuelve el Top 10.
    f"Trump's speech 10 most frequent words:\n"
    f"{find_most_common_words('./data/donald_speech.txt', 10)}\n\n"
    
    # 6. √öltima ejecuci√≥n para el discurso de Melina. 
    # Cerramos el par√©ntesis del print despu√©s de haber concatenado todas las cadenas.
    f"Melina's speech 10 most frequent words:\n"
    f"{find_most_common_words('./data/melina_trump_speech.txt', 10)}"
)

#  TODAVIA NO HE VISTO REGEX ASI QUE LO DEJO PARA MAS ADELANTE
#
# ==========================================
# 7. Escriba una aplicaci√≥n que compruebe la similitud entre dos textos (Michelle vs Melina).
# Necesitar√°s limpiar el texto, eliminar stop_words y calcular la similitud.

# 1. Funci√≥n para extraer palabras limpias de un archivo.
# Devuelve una lista de strings con todas las palabras encontradas.
def clean_text(file: str) -> list[str]:
    # Abrimos el archivo en modo lectura.
    with open(file, "r") as f:
        # Leemos todo el contenido y lo pasamos a min√∫sculas para normalizar.
        raw_text: str = f.read().lower()

    # Usamos Regex para extraer solo secuencias de letras, ignorando signos y n√∫meros.
    # '\b' marca el l√≠mite de palabra para asegurar capturas precisas.
    return re.findall(r"\b[a-z]+\b", raw_text)

# 2. Funci√≥n para filtrar las "stop words" (palabras de soporte como 'the', 'is', 'of').
def remove_support_words(all_words: list[str]) -> list[str]:
    # Usamos una List Comprehension para crear una nueva lista.
    # Solo incluimos la palabra si NO est√° en la lista global de stop_words 'sw'.
    return [word for word in all_words if word not in sw]

# 3. Funci√≥n principal para calcular el √≠ndice de similitud de Jaccard.
# Recibe las rutas de dos archivos y devuelve un n√∫mero entre 0.0 (nada parecido) y 1.0 (id√©nticos).
def check_txt_similarity(text_path1: str, text_path2: str) -> float:
    # Procesamos el primer texto: limpiamos -> quitamos stop_words -> convertimos a SET (conjunto).
    # El set() es clave porque elimina duplicados y permite operaciones matem√°ticas de conjuntos.
    words1: set[str] = set(remove_support_words(clean_text(text_path1)))
    
    # Hacemos lo mismo con el segundo texto.
    words2: set[str] = set(remove_support_words(clean_text(text_path2)))
    
    # Calculamos la INTERSECCI√ìN: palabras que aparecen en AMBOS conjuntos.
    intersection: set[str] = words1.intersection(words2)
    
    # Calculamos la UNI√ìN: todas las palabras √∫nicas que existen entre los dos textos.
    union: set[str] = words1.union(words2)
    
    # Aplicamos la f√≥rmula de Jaccard: (elementos comunes) / (total de elementos √∫nicos).
    # Usamos 'round(..., 2)' para limitar el resultado a dos decimales.
    # 'if union else 0.0' evita el error de divisi√≥n por cero si ambos archivos est√°n vac√≠os.
    return round(len(intersection) / len(union), 2) if union else 0.0

# 8. Encuentra las 10 palabras m√°s repetidas en romeo_and_juliet.txt.
print(f"The 10 most repeated words in romeo and juliet are: {find_most_common_words("./data/romeo_and_juliet.txt", 10)}")

# 9. Lea el archivo hacker_news.csv y averig√ºe:
# a) N√∫mero de l√≠neas que contienen Python o python
# b) N√∫mero de l√≠neas que contienen JavaScript, Javascript o javascript
# c) N√∫mero de l√≠neas que contienen Java y NO JavaScript

import csv  # Importamos el m√≥dulo csv para poder leer archivos CSV correctamente

# Definimos una funci√≥n que recibe la ruta del archivo CSV como string
def count_languages(file_path: str):
    # Contador de l√≠neas que contienen la palabra "python"
    py_count = 0

    # Contador de l√≠neas que contienen la palabra "javascript"
    js_count = 0

    # Contador de l√≠neas que contienen "java" pero NO "javascript"
    java_count = 0
    
    # Abrimos el archivo CSV en modo lectura ("r")
    # with asegura que el archivo se cierre autom√°ticamente al terminar
    with open(file_path, "r") as csvf:
        
        # csv.reader permite leer el archivo l√≠nea por l√≠nea,
        # donde cada l√≠nea se devuelve como una lista de valores (columnas)
        #Est√°s creando un objeto lector (iterable). 
        #Es como un puntero que est√° listo para recorrer el archivo fila por fila.
        # id	        title	                    url	                num_points
        # 1	        Learning Python is fun	    http://python.org	        100
        # 2	        JS vs Java: The battle	    http://battle.com	        50
        # 3	        Java for beginners	        http://java.com	            80

        csvreader = csv.reader(csvf)

        # Recorremos cada fila (row) del archivo CSV
        for row in csvreader:
            
            #Vuelta1: row es una lista (por ejemplo: ["title", "url", "author", ...])
            #Vuelta2: row recibe ['1', 'Learning Python is fun', 'http://python.org', '100']
            #VUelta3: row recibe ['2', 'JavaScript is great', 'http://js.com', '150']
            # " ".join(row) une todos los elementos de la fila en un solo string
            # .lower() convierte todo a min√∫sculas para evitar problemas de may√∫sculas
            line = " ".join(row).lower()
            
            # Si la palabra "python" aparece en la l√≠nea,
            # aumentamos el contador de Python
            if "python" in line:
                py_count += 1
            
            # Si la palabra "javascript" aparece en la l√≠nea,
            # aumentamos el contador de JavaScript
            if "javascript" in line:
                js_count += 1
            
            # Si la palabra "java" aparece en la l√≠nea
            # Y al mismo tiempo "javascript" NO aparece,
            # entonces contamos solo Java (excluyendo JavaScript)
            if "java" in line and "javascript" not in line:
                java_count += 1
    
    # Devolvemos un string con los resultados finales formateados
    return (
        f"Python count: {py_count}\n"
        f"JavaScript count: {js_count}\n"
        f"Java (not JS) count: {java_count}"
    )

# Llamamos a la funci√≥n pasando la ruta del archivo CSV
# e imprimimos el resultado en pantalla
print(count_languages("./data/hacker_news.csv"))
